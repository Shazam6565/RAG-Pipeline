/**
 * @public
 */
export declare type DiagnosticsLogEvent = {
    timestamp: number;
    data: DiagnosticsLogEventData;
};

/**
 * @public
 */
export declare type DiagnosticsLogEventData = {
    type: "llm.prediction.input";
    modelPath: string;
    modelIdentifier: string;
    input: string;
};

/** @public */
export declare class DiagnosticsNamespace {
    private readonly diagnosticsPort;
    private readonly validator;
    /**
     * Register a callback to receive log events. Return a function to stop receiving log events.
     *
     * This method is in alpha. Do not use this method in production yet.
     * @alpha
     */
    unstable_streamLogs(listener: (logEvent: DiagnosticsLogEvent) => void): () => void;
}

/**
 * Represents a model that exists locally and can be loaded.
 *
 * @public
 */
export declare type DownloadedModel = {
    /**
     * The type of the model.
     */
    type: "llm" | "embedding";
    /**
     * The path of the model. Use to load the model.
     */
    path: string;
    /**
     * The size of the model in bytes.
     */
    sizeBytes: number;
    /**
     * The architecture of the model.
     */
    architecture?: string;
};

/**
 * How much of the model's work should be offloaded to the GPU. The value should be between 0 and 1.
 * A value of 0 means that no layers are offloaded to the GPU, while a value of 1 means that all
 * layers (that can be offloaded) are offloaded to the GPU.
 *
 * Alternatively, the value can be set to "auto", which means it will be determined automatically.
 * (Currently uses the value in the preset.)
 *
 * @public
 */
export declare type LLMAccelerationOffload = number | "auto" | "max" | "off";

/**
 * Represents the history of a conversation, which is represented as an array of messages.
 *
 * @public
 */
export declare type LLMChatHistory = Array<LLMChatHistoryMessage>;

/**
 * Represents a single message in the history.
 *
 * @public
 */
export declare interface LLMChatHistoryMessage {
    role: LLMChatHistoryRole;
    content: string;
}

/**
 * Represents a role in a specific message in the history. This is a string enum, and can only be
 * one of the following values:
 *
 * - `system`: Usually used for system prompts
 * - `user`: Used for user inputs / queries
 * - `assistant`: Used for assistant responses, usually generated AI, but can also be fed by a human
 *
 * @public
 */
export declare type LLMChatHistoryRole = string;

/** @public */
export declare interface LLMChatPredictionConfig extends LLMPredictionConfigBase {
    /**
     * A string that will be prepended to each of the user message.
     */
    inputPrefix?: string;
    /**
     * A string that will be appended to each of the user message.
     */
    inputSuffix?: string;
}

/** @public */
export declare interface LLMChatResponseOpts extends LLMChatPredictionConfig {
    /**
     * Structured output settings for the prediction. See {@link LLMStructuredPredictionSetting} for a
     * detailed explanation of what structured prediction is and how to use it.
     */
    structured?: LLMStructuredPredictionSetting;
}

/** @public */
export declare interface LLMCompletionOpts extends LLMCompletionPredictionConfig {
    /**
     * Structured output settings for the prediction. See {@link LLMStructuredPredictionSetting} for a
     * detailed explanation of what structured prediction is and how to use it.
     */
    structured?: LLMStructuredPredictionSetting;
}

/**
 * Extra config options for a `complete` prediction.
 *
 * @public
 */
export declare interface LLMCompletionPredictionConfig extends LLMPredictionConfigBase {
    /**
     * The pre-prompt to use for the prediction. The pre-prompt will be formatted and prepended to the
     * input before making a prediction.
     */
    prePrompt?: string;
}

/**
 * Behavior for when the generated tokens length exceeds the context window size. Only the following
 * values are allowed:
 *
 * - `stopAtLimit`: Stop the prediction when the generated tokens length exceeds the context window
 *   size. If the generation is stopped because of this limit, the `stopReason` in the prediction
 *   stats will be set to `contextLengthReached`.
 * - `truncateMiddle`: Keep the system prompt and the first user message, truncate middle.
 * - `rollingWindow`: Maintain a rolling window and truncate past messages.
 *
 * @public
 */
export declare type LLMContextOverflowPolicy = "stopAtLimit" | "truncateMiddle" | "rollingWindow";

/**
 * Describes a specific loaded LLM.
 *
 * @public
 */
export declare interface LLMDescriptor {
    /**
     * The identifier of the LLM (Set when loading the model. Defaults to the same as the path.)
     *
     * Identifier identifies a currently loaded model.
     */
    identifier: string;
    /**
     * The path of the LLMModel. (i.e. which model is this)
     *
     * An path is associated with a specific model that can be loaded.
     */
    path: string;
}

/**
 * This represents a set of requirements for a model. It is not tied to a specific model, but rather
 * to a set of requirements that a model must satisfy.
 *
 * For example, if you got the model via `client.llm.get("my-identifier")`, you will get a
 * `LLMModel` for the model with the identifier `my-identifier`. If the model is unloaded, and
 * another model is loaded with the same identifier, using the same `LLMModel` will use the new
 * model.
 *
 * @public
 */
export declare class LLMDynamicHandle {
    /**
     * Use the loaded model to predict text.
     *
     * This method returns an {@link OngoingPrediction} object. An ongoing prediction can be used as a
     * promise (if you only care about the final result) or as an async iterable (if you want to
     * stream the results as they are being generated).
     *
     * Example usage as a promise (Resolves to a {@link PredictionResult}):
     *
     * ```typescript
     * const result = await model.complete("When will The Winds of Winter be released?");
     * console.log(result.content);
     * ```
     *
     * Or
     *
     * ```typescript
     * model.complete("When will The Winds of Winter be released?")
     *  .then(result =\> console.log(result.content))
     *  .catch(error =\> console.error(error));
     * ```
     *
     * Example usage as an async iterable (streaming):
     *
     * ```typescript
     * for await (const fragment of model.complete("When will The Winds of Winter be released?")) {
     *   process.stdout.write(fragment);
     * }
     * ```
     *
     * If you wish to stream the result, but also getting the final prediction results (for example,
     * you wish to get the prediction stats), you can use the following pattern:
     *
     * ```typescript
     * const prediction = model.complete("When will The Winds of Winter be released?");
     * for await (const fragment of prediction) {
     *   process.stdout.write(fragment);
     * }
     * const result = await prediction;
     * console.log(result.stats);
     * ```
     *
     * @param prompt - The prompt to use for prediction.
     * @param opts - Options for the prediction.
     */
    complete(prompt: string, opts?: LLMCompletionOpts): OngoingPrediction;
    /**
     * Use the loaded model to generate a response based on the given history.
     *
     * This method returns an {@link OngoingPrediction} object. An ongoing prediction can be used as a
     * promise (if you only care about the final result) or as an async iterable (if you want to
     * stream the results as they are being generated).
     *
     * Example usage as a promise (Resolves to a {@link PredictionResult}):
     *
     * ```typescript
     * const history = [{ type: 'user', content: "When will The Winds of Winter be released?" }];
     * const result = await model.respond(history);
     * console.log(result.content);
     * ```
     *
     * Or
     *
     * ```typescript
     * const history = [{ type: 'user', content: "When will The Winds of Winter be released?" }];
     * model.respond(history)
     *  .then(result => console.log(result.content))
     *  .catch(error => console.error(error));
     * ```
     *
     * Example usage as an async iterable (streaming):
     *
     * ```typescript
     * const history = [{ type: 'user', content: "When will The Winds of Winter be released?" }];
     * for await (const fragment of model.respond(history)) {
     *   process.stdout.write(fragment);
     * }
     * ```
     *
     * If you wish to stream the result, but also getting the final prediction results (for example,
     * you wish to get the prediction stats), you can use the following pattern:
     *
     * ```typescript
     * const history = [{ type: 'user', content: "When will The Winds of Winter be released?" }];
     * const prediction = model.respond(history);
     * for await (const fragment of prediction) {
     *   process.stdout.write(fragment);
     * }
     * const result = await prediction;
     * console.log(result.stats);
     * ```
     *
     * @param history - The LLMChatHistory array to use for generating a response.
     * @param opts - Options for the prediction.
     */
    respond(history: LLMChatHistory, opts?: LLMChatResponseOpts): OngoingPrediction;
    /**
     * Gets the information of the model that is currently associated with this `LLMModel`. If no
     * model is currently associated, this will return `undefined`.
     *
     * Note: As models are loaded/unloaded, the model associated with this `LLMModel` may change at
     * any moment.
     */
    getModelInfo(): Promise<LLMDescriptor | undefined>;
}

/** @public */
export declare interface LLMLoadModelConfig {
    /**
     * The size of the context length in number of tokens. This will include both the prompts and the
     * responses. Once the context length is exceeded, the value set in
     * {@link LLMPredictionConfigBase#contextOverflowPolicy} is used to determine the behavior.
     *
     * See {@link LLMContextOverflowPolicy} for more information.
     */
    contextLength?: number;
    /**
     * How much of the model's work should be offloaded to the GPU. The value should be between 0 and 1.
     * A value of 0 means that no layers are offloaded to the GPU, while a value of 1 means that all
     * layers (that can be offloaded) are offloaded to the GPU.
     *
     * Alternatively, the value can be set to "auto", which means it will be determined automatically.
     * (Currently uses the value in the preset.)
     *
     * @public
     */
    gpuOffload?: LLMAccelerationOffload;
}

/** @public */
export declare interface LLMLoadModelOpts {
    /**
     * The name of the preset to use when loading the model. Preset can be downloaded/created/edited
     * in the LM Studio application.
     *
     * Presets act as the default configuration for the model. That is, when you load or inference
     * using a LLM model, the preset's configuration is used unless you override it with the `config`
     * option. Overriding happens on a per-field basis, so you can override only the fields you want.
     *
     * If no preset is specified, LM Studio will use the default preset selected for the model in the
     * My Models page. If no preset is selected for that model, the default preset for the operating
     * system is used. ("Default LM Studio macOS" or "Default LM Studio Windows).
     */
    preset?: string;
    /**
     * The identifier to use for the loaded model.
     *
     * By default, the identifier is the same as the path (1st parameter). If the identifier already
     * exists, a number will be attached. This option allows you to specify the identifier to use.
     *
     * However, when the identifier is specified and it is in use, an error will be thrown. If the
     * call is successful, it is guaranteed that the loaded model will have the specified identifier.
     */
    identifier?: string;
    /**
     * The configuration to use when loading the model. See {@link LLMLoadModelConfig} for details. By
     * default, the model is loaded with the configuration specified in the preset.
     *
     * If no preset is specified, LM Studio will use the default preset selected for the model in the
     * My Models page. If no preset is selected for that model, the default preset for the operating
     * system is used. ("Default LM Studio macOS" or "Default LM Studio Windows).
     */
    config?: LLMLoadModelConfig;
    /**
     * An `AbortSignal` to cancel the model loading. This is useful if you wish to add a functionality
     * to cancel the model loading.
     *
     * Example usage:
     *
     * ```typescript
     * const ac = new AbortController();
     * const model = await client.llm.load({
     *   model: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF",
     *   signal: ac.signal,
     * });
     *
     * // Later, to cancel the model loading
     * ac.abort();
     * ```
     *
     * AbortController/AbortSignal is the standard method for cancelling an asynchronous operation in
     * JavaScript. For more information, visit
     * https://developer.mozilla.org/en-US/docs/Web/API/AbortController
     */
    signal?: AbortSignal;
    /**
     * Controls the logging of model loading progress.
     *
     * - If set to `true`, logs progress at the "info" level.
     * - If set to `false`, no logs are emitted. This is the default.
     * - If a specific logging level is desired, it can be provided as a string. Acceptable values are
     *   "debug", "info", "warn", and "error".
     *
     * Logs are directed to the logger specified during the `LMStudioClient` construction.
     *
     * Progress logs will be disabled if an `onProgress` callback is provided.
     *
     * Default value is "info", which logs progress at the "info" level.
     */
    verbose?: boolean | LogLevel;
    /**
     * A function that is called with the progress of the model loading. The function is called with a
     * number between 0 and 1, inclusive, representing the progress of the model loading.
     *
     * If an `onProgress` callback is provided, verbose progress logs will be disabled.
     */
    onProgress?: (progress: number) => void;
    /**
     * By default, the model will automatically be unloaded when the last client with the same
     * `clientIdentifier` disconnects. If you set this option to `true`, the model will not be
     * automatically unloaded.
     *
     * This especially useful when you are a sysadmin and you are trying to make a model ready using
     * scripts. In this case, you can set this option to `true`, so you do not need to keep the client
     * connected.
     */
    noHup?: boolean;
}

/**
 * Represents a query for a loaded LLM.
 *
 * @public
 */
export declare interface LLMModelQuery {
    /**
     * If specified, the model must have exactly this identifier.
     *
     * Note: The identifier of a model is set when loading the model. It defaults to the filename of
     * the model if not specified. However, this default behavior should not be relied upon. If you
     * wish to query a model by its path, you should specify the path instead of the identifier:
     *
     * Instead of
     *
     * ```ts
     * const model = client.llm.get({ identifier: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF" });
     * // OR
     * const model = client.llm.get("lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF");
     * ```
     *
     * Use
     *
     * ```ts
     * const model = client.llm.get({ path: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF" });
     * ```
     */
    identifier?: string;
    /**
     * If specified, the model must have this path.
     *
     * When specifying the model path, you can use the following format:
     *
     * `<publisher>/<repo>[/model_file]`
     *
     * If `model_file` is not specified, any quantization of the model will match this query.
     *
     * Here are some examples:
     *
     * Query any loaded Llama 3 model:
     *
     * ```ts
     * const model = client.llm.get({
     *   path: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF",
     * });
     * ```
     *
     * Query any loaded model with a specific quantization of the Llama 3 model:
     *
     * ```ts
     * const model = client.llm.get({
     *   path: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
     * });
     * ```
     */
    path?: string;
}

/** @public */
export declare class LLMNamespace {
    private readonly llmPort;
    private readonly validator;
    /**
     * Load a model for inferencing. The first parameter is the model path. The second parameter is
     * an optional object with additional options. By default, the model is loaded with the default
     * preset (as selected in LM Studio) and the verbose option is set to true.
     *
     * When specifying the model path, you can use the following format:
     *
     * `<publisher>/<repo>[/model_file]`
     *
     * If `model_file` is not specified, the first (sorted alphabetically) model in the repository is
     * loaded.
     *
     * To find out what models are available, you can use the `lms ls` command, or programmatically
     * use the `client.system.listDownloadedModels` method.
     *
     * Here are some examples:
     *
     * Loading Llama 3:
     *
     * ```typescript
     * const model = await client.llm.load("lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF");
     * ```
     *
     * Loading a specific quantization (q4_k_m) of Llama 3:
     *
     * ```typescript
     * const model = await client.llm.load("lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf");
     * ```
     *
     * To unload the model, you can use the `client.llm.unload` method. Additionally, when the last
     * client with the same `clientIdentifier` disconnects, all models loaded by that client will be
     * automatically unloaded.
     *
     * Once loaded, see {@link LLMDynamicHandle} for how to use the model for inferencing or other things you
     * can do with the model.
     *
     * @param path - The path of the model to load. See {@link LLMLoadModelOpts} for
     * details.
     * @param opts - Options for loading the model. See {@link LLMLoadModelOpts} for details.
     * @returns A promise that resolves to the model that can be used for inferencing
     */
    load(path: string, opts?: LLMLoadModelOpts): Promise<LLMDynamicHandle>;
    /**
     * Unload a model. Once a model is unloaded, it can no longer be used. If you wish to use the
     * model afterwards, you will need to load it with {@link LLMNamespace#loadModel} again.
     *
     * @param identifier - The identifier of the model to unload.
     */
    unload(identifier: string): Promise<void>;
    /**
     * List all the currently loaded models.
     */
    listLoaded(): Promise<Array<LLMDescriptor>>;
    /**
     * Get a specific model that satisfies the given query. The returned model is tied to the specific
     * model at the time of the call.
     *
     * For more information on the query, see {@link LLMModelQuery}.
     *
     * @example
     *
     * If you have loaded a model with the identifier "my-model", you can use it like this:
     *
     * ```ts
     * const model = await client.llm.get({ identifier: "my-model" });
     * const prediction = model.complete("...");
     * ```
     *
     * Or just
     *
     * ```ts
     * const model = await client.llm.get("my-model");
     * const prediction = model.complete("...");
     * ```
     *
     * @example
     *
     * Use the Gemma 2B IT model (given it is already loaded elsewhere):
     *
     * ```ts
     * const model = await client.llm.get({ path: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF" });
     * const prediction = model.complete("...");
     * ```
     */
    get(query: LLMModelQuery): Promise<LLMSpecificModel>;
    /**
     * Get a specific model by its identifier. The returned model is tied to the specific model at the
     * time of the call.
     *
     * @example
     *
     * If you have loaded a model with the identifier "my-model", you can use it like this:
     *
     * ```ts
     * const model = await client.llm.get("my-model");
     * const prediction = model.complete("...");
     * ```
     *
     */
    get(path: string): Promise<LLMSpecificModel>;
    /**
     * Get a dynamic model handle for any loaded model that satisfies the given query.
     *
     * For more information on the query, see {@link LLMModelQuery}.
     *
     * Note: The returned `LLMModel` is not tied to any specific loaded model. Instead, it represents
     * a "handle for a model that satisfies the given query". If the model that satisfies the query is
     * unloaded, the `LLMModel` will still be valid, but any method calls on it will fail. And later,
     * if a new model is loaded that satisfies the query, the `LLMModel` will be usable again.
     *
     * You can use {@link LLMDynamicHandle#getModelInfo} to get information about the model that is
     * currently associated with this handle.
     *
     * @example
     *
     * If you have loaded a model with the identifier "my-model", you can use it like this:
     *
     * ```ts
     * const dh = client.llm.createDynamicHandle({ identifier: "my-model" });
     * const prediction = dh.complete("...");
     * ```
     *
     * @example
     *
     * Use the Gemma 2B IT model (given it is already loaded elsewhere):
     *
     * ```ts
     * const dh = client.llm.createDynamicHandle({ path: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF" });
     * const prediction = dh.complete("...");
     * ```
     *
     * @param query - The query to use to get the model.
     */
    createDynamicHandle(query: LLMModelQuery): LLMDynamicHandle;
    /**
     * Get a dynamic model handle by its identifier.
     *
     * Note: The returned `LLMModel` is not tied to any specific loaded model. Instead, it represents
     * a "handle for a model with the given identifier". If the model with the given identifier is
     * unloaded, the `LLMModel` will still be valid, but any method calls on it will fail. And later,
     * if a new model is loaded with the same identifier, the `LLMModel` will be usable again.
     *
     * You can use {@link LLMDynamicHandle#getModelInfo} to get information about the model that is
     * currently associated with this handle.
     *
     * @example
     *
     * If you have loaded a model with the identifier "my-model", you can get use it like this:
     *
     * ```ts
     * const dh = client.llm.createDynamicHandle("my-model");
     * const prediction = dh.complete("...");
     * ```
     *
     * @param identifier - The identifier of the model to get.
     */
    createDynamicHandle(identifier: string): LLMDynamicHandle;
}

/**
 * Shared config for running predictions on an LLM.
 *
 * @public
 */
export declare interface LLMPredictionConfigBase {
    /**
     * Number of tokens to predict at most. If set to -1, the model will predict as many tokens as it
     * wants.
     *
     * When the prediction is stopped because of this limit, the `stopReason` in the prediction stats
     * will be set to `maxPredictedTokensReached`.
     *
     * See {@link LLMPredictionStopReason} for other reasons that a prediction might stop.
     */
    maxPredictedTokens?: number;
    /**
     * The temperature parameter for the prediction model. A higher value makes the predictions more
     * random, while a lower value makes the predictions more deterministic. The value should be
     * between 0 and 1.
     */
    temperature?: number;
    /**
     * An array of strings. If the model generates one of these strings, the prediction will stop.
     *
     * When the prediction is stopped because of this limit, the `stopReason` in the prediction stats
     * will be set to `stopStringFound`.
     *
     * See {@link LLMPredictionStopReason} for other reasons that a prediction might stop.
     */
    stopStrings?: Array<string>;
    /**
     * The behavior for when the generated tokens length exceeds the context window size. The allowed
     * values are:
     *
     * - `stopAtLimit`: Stop the prediction when the generated tokens length exceeds the context
     *   window size. If the generation is stopped because of this limit, the `stopReason` in the
     *   prediction stats will be set to `contextLengthReached`
     * - `truncateMiddle`: Keep the system prompt and the first user message, truncate middle.
     * - `rollingWindow`: Maintain a rolling window and truncate past messages.
     */
    contextOverflowPolicy?: LLMContextOverflowPolicy;
}

/** @public */
export declare interface LLMPredictionStats {
    /**
     * The reason why the prediction stopped.
     *
     * This is a string enum with the following possible values:
     *
     * - `userStopped`: The user stopped the prediction. This includes calling the `cancel` method on
     *   the `OngoingPrediction` object.
     * - `modelUnloaded`: The model was unloaded during the prediction.
     * - `failed`: An error occurred during the prediction.
     * - `eosFound`: The model predicted an end-of-sequence token, which is a way for the model to
     *   indicate that it "thinks" the sequence is complete.
     * - `stopStringFound`: A stop string was found in the prediction. (Stop strings can be specified
     *   with the `stopStrings` config option. This stop reason will only occur if the `stopStrings`
     *   config option is set.)
     * - `maxPredictedTokensReached`: The maximum number of tokens to predict was reached. (Length
     *   limit can be specified with the `maxPredictedTokens` config option. This stop reason will
     *   only occur if the `maxPredictedTokens` config option is set to a value other than -1.)
     * - `contextLengthReached`: The context length was reached. This stop reason will only occur if
     *   the `contextOverflowPolicy` is set to `stopAtLimit`.
     */
    stopReason: LLMPredictionStopReason;
    /**
     * The average number of tokens predicted per second.
     *
     * Note: This value can be undefined in the case of a very short prediction which results in a
     * NaN or a Infinity value.
     */
    tokensPerSecond?: number;
    /**
     * The number of GPU layers used in the prediction.
     */
    numGpuLayers?: number;
    /**
     * The time it took to predict the first token in seconds.
     */
    timeToFirstTokenSec?: number;
    /**
     * The number of tokens that were supplied.
     */
    promptTokensCount?: number;
    /**
     * The number of tokens that were predicted.
     */
    predictedTokensCount?: number;
    /**
     * The total number of tokens. This is the sum of the prompt tokens and the predicted tokens.
     */
    totalTokensCount?: number;
}

/**
 * Represents the reason why a prediction stopped. Only the following values are possible:
 *
 * - `userStopped`: The user stopped the prediction. This includes calling the `cancel` method on
 *   the `OngoingPrediction` object.
 * - `modelUnloaded`: The model was unloaded during the prediction.
 * - `failed`: An error occurred during the prediction.
 * - `eosFound`: The model predicted an end-of-sequence token, which is a way for the model to
 *   indicate that it "thinks" the sequence is complete.
 * - `stopStringFound`: A stop string was found in the prediction. (Stop strings can be specified
 *   with the `stopStrings` config option. This stop reason will only occur if the `stopStrings`
 *   config option is set to an array of strings.)
 * - `maxPredictedTokensReached`: The maximum number of tokens to predict was reached. (Length limit
 *   can be specified with the `maxPredictedTokens` config option. This stop reason will only occur
 *   if the `maxPredictedTokens` config option is set to a value other than -1.)
 * - `contextLengthReached`: The context length was reached. This stop reason will only occur if the
 *   `contextOverflowPolicy` is set to `stopAtLimit`.
 *
 * @public
 */
export declare type LLMPredictionStopReason = "userStopped" | "modelUnloaded" | "failed" | "eosFound" | "stopStringFound" | "maxPredictedTokensReached" | "contextLengthReached";

/**
 * Represents a specific loaded model. Most LLM related operations are inherited from
 * {@link LLMDynamicHandle}.
 *
 * @public
 */
export declare class LLMSpecificModel extends LLMDynamicHandle {
    readonly identifier: string;
    readonly path: string;
}

/**
 * Settings for structured prediction. Structured prediction is a way to force the model to generate
 * predictions that conform to a specific structure.
 *
 * For example, you can use structured prediction to make the model only generate valid JSON, or
 * event JSON that conforms to a specific schema (i.e. having strict types).
 *
 * Some examples:
 *
 * Only generate valid JSON:
 *
 * ```ts
 * const prediction = model.complete("...", {
 *   maxPredictedTokens: 100,
 *   structured: { type: "json" },
 * });
 * ```
 *
 * Only generate JSON that conforms to a specific schema (See https://json-schema.org/ for more
 * information on authoring JSON schema):
 *
 * ```ts
 * const schema = {
 *   type: "object",
 *   properties: {
 *     name: { type: "string" },
 *     age: { type: "number" },
 *   },
 *   required: ["name", "age"],
 * };
 * const prediction = model.complete("...", {
 *   maxPredictedTokens: 100,
 *   structured: { type: "json", jsonSchema: schema },
 * });
 * ```
 *
 * By default, `{ type: "none" }` is used, which means no structured prediction is used.
 *
 * Caveats:
 *
 * - Although the model is forced to generate predictions that conform to the specified structure,
 *   the prediction may be interrupted (for example, if the user stops the prediction). When that
 *   happens, the partial result may not conform to the specified structure. Thus, always check the
 *   prediction result before using it, for example, by wrapping the `JSON.parse` inside a try-catch
 *   block.
 * - In certain cases, the model may get stuck. For example, when forcing it to generate valid JSON,
 *   it may generate a opening brace `{` but never generate a closing brace `}`. In such cases, the
 *   prediction will go on forever until the context length is reached, which can take a long time.
 *   Therefore, it is recommended to always set a `maxPredictedTokens` limit.
 *
 * @public
 */
export declare type LLMStructuredPredictionSetting = {
    type: "none";
} | {
    type: "json";
    jsonSchema?: any;
};

/** @public */
export declare class LMStudioClient {
    readonly clientIdentifier: string;
    readonly llm: LLMNamespace;
    readonly system: SystemNamespace;
    readonly diagnostics: DiagnosticsNamespace;
    private isLocalhostWithGivenPortLMStudioServer;
    /**
     * Guess the base URL of the LM Studio server by visiting localhost on various default ports.
     */
    private guessBaseUrl;
    constructor(opts?: LMStudioClientConstructorOpts);
}

/** @public */
export declare interface LMStudioClientConstructorOpts {
    /**
     * Changes the logger that is used by LMStudioClient internally. The default logger is `console`.
     * By default, LMStudioClient only logs warnings and errors that require user intervention. If the
     * `verbose` option is enabled while calling supporting methods, those messages will also be
     * directed to the specified logger.
     */
    logger?: LoggerInterface;
    /**
     * The base URL of the LM Studio server. If not provided, LM Studio will attempt to connect to the
     * localhost with various default ports.
     *
     * If you have set a custom port and/or are reverse-proxying, you should pass in the baseUrl.
     *
     * Since LM Studio uses WebSockets, the protocol must be "ws" or "wss".
     *
     * For example, if have changed the port to 8080, you should create the LMStudioClient like so:
     *
     * ```typescript
     * const client = new LMStudioClient({ baseUrl: "ws://127.0.0.1:8080" });
     * ```
     */
    baseUrl?: string;
    /**
     * Whether to include stack traces in the errors caused by LM Studio. By default, this is set to
     * `false`. If set to `true`, LM Studio SDK will include a stack trace in the error message.
     */
    verboseErrorMessages?: boolean;
    /**
     * Changes the client identifier used to authenticate with LM Studio. By default, it uses a
     * randomly generated string.
     *
     * If you wish to share resources across multiple LMStudioClient, you should set them to use the
     * same `clientIdentifier` and `clientPasskey`.
     */
    clientIdentifier?: string;
    /**
     * Changes the client passkey used to authenticate with LM Studio. By default, it uses a randomly
     * generated string.
     *
     * If you wish to share resources across multiple LMStudioClient, you should set them to use the
     * same `clientIdentifier` and `clientPasskey`.
     */
    clientPasskey?: string;
}

/** @public */
export declare interface LoggerInterface {
    info(...messages: Array<unknown>): void;
    error(...messages: Array<unknown>): void;
    warn(...messages: Array<unknown>): void;
    debug(...messages: Array<unknown>): void;
}

/** @public */
export declare type LogLevel = "debug" | "info" | "warn" | "error";

/**
 * Represents an ongoing prediction.
 *
 * Note, this class is Promise-like, meaning you can use it as a promise. It resolves to a
 * {@link PredictionResult}, which contains the generated text in the `.content` property. Example
 * usage:
 *
 * ```typescript
 * const result = await model.complete("When will The Winds of Winter be released?");
 * console.log(result.content);
 * ```
 *
 * Or you can use instances methods like `then` and `catch` to handle the result or error of the
 * prediction.
 *
 * ```typescript
 * model.complete("When will The Winds of Winter be released?")
 *  .then(result =\> console.log(result.content))
 *  .catch(error =\> console.error(error));
 * ```
 *
 * Alternatively, you can also stream the result (process the results as more content is being
 * generated). For example:
 *
 * ```typescript
 * for await (const fragment of model.complete("When will The Winds of Winter be released?")) {
 *   process.stdout.write(fragment);
 * }
 * ```
 *
 * @public
 */
export declare class OngoingPrediction extends StreamablePromise<string, PredictionResult> {
    private readonly onCancel;
    private stats;
    private modelInfo;
    protected collect(fragments: ReadonlyArray<string>): Promise<PredictionResult>;
    private constructor();
    /**
     * Get the final prediction results. If you have been streaming the results, awaiting on this
     * method will take no extra effort, as the results are already available in the internal buffer.
     *
     * Example:
     *
     * ```typescript
     * const prediction = model.complete("When will The Winds of Winter be released?");
     * for await (const fragment of prediction) {
     *   process.stdout.write(fragment);
     * }
     * const result = await prediction.result();
     * console.log(result.stats);
     * ```
     *
     * Technically, awaiting on this method is the same as awaiting on the instance itself:
     *
     * ```typescript
     * await prediction.result();
     *
     * // Is the same as:
     *
     * await prediction;
     * ```
     */
    result(): Promise<PredictionResult>;
    /**
     * Cancels the prediction. This will stop the prediction with stop reason `userStopped`. See
     * {@link LLMPredictionStopReason} for other reasons that a prediction might stop.
     */
    cancel(): Promise<void>;
}

/**
 * Represents the result of a prediction.
 *
 * The most notably property is {@link PredictionResult#content}, which contains the generated text.
 * Additionally, the {@link PredictionResult#stats} property contains statistics about the
 * prediction.
 *
 * @public
 */
export declare class PredictionResult {
    /**
     * The newly generated text as predicted by the LLM.
     */
    readonly content: string;
    /**
     * Statistics about the prediction.
     */
    readonly stats: LLMPredictionStats;
    /**
     * Information about the model used for the prediction.
     */
    readonly modelInfo: LLMDescriptor;
    constructor(
    /**
     * The newly generated text as predicted by the LLM.
     */
    content: string, 
    /**
     * Statistics about the prediction.
     */
    stats: LLMPredictionStats, 
    /**
     * Information about the model used for the prediction.
     */
    modelInfo: LLMDescriptor);
}

/**
 * A StreamablePromise is a promise-like that is also async iterable. This means you can use it as a
 * promise (awaiting it, using `.then`, `.catch`, etc.), and you can also use it as an async
 * iterable (using `for await`).
 *
 * Notably, as much as it implements the async iterable interface, it is not a traditional iterable,
 * as it internally maintains a buffer and new values are pushed into the buffer by the producer, as
 * oppose to being pulled by the consumer.
 *
 * The async iterable interface is used instead of the Node.js object stream because streams are too
 * clunky to use, and the `for await` syntax is much more ergonomic for most people.
 *
 * If any iterator is created for this instance, an empty rejection handler will be attached to the
 * promise to prevent unhandled rejection warnings.
 *
 * This class is provided as an abstract class and is meant to be extended. Crucially, the `collect`
 * method must be implemented, which will be called to convert an array of values into the final
 * resolved value of the promise.
 *
 * In addition, the constructor of the subclass should be marked as private, and a static method
 * that exposes the constructor, the `finished` method, and the `push` method should be provided.
 *
 * @typeParam TFragment - The type of the individual fragments that are pushed into the buffer.
 * @typeParam TFinal - The type of the final resolved value of the promise.
 * @public
 */
export declare abstract class StreamablePromise<TFragment, TFinal> implements Promise<TFinal>, AsyncIterable<TFragment> {
    protected abstract collect(fragments: ReadonlyArray<TFragment>): Promise<TFinal>;
    private promiseFinal;
    private resolveFinal;
    private rejectFinal;
    protected status: "pending" | "resolved" | "rejected";
    private buffer;
    private nextFragmentPromiseBundle;
    /**
     * If there has ever been any iterators created for this instance. Once any iterator is created,
     * a reject handler will be attached to the promise to prevent unhandled rejection warnings, as
     * the errors will be handled by the iterator.
     *
     * The purpose of this variable is to prevent registering the reject handler more than once.
     */
    private hasIterator;
    /**
     * Called by the producer when it has finished producing values. If an error is provided, the
     * promise will be rejected with that error. If no error is provided, the promise will be resolved
     * with the final value.
     *
     * This method should be exposed in the static constructor of the subclass.
     *
     * @param error - The error to reject the promise with, if any.
     */
    protected finished(error?: any): void;
    /**
     * Called by the producer to push a new fragment into the buffer. This method should be exposed in
     * the static constructor of the subclass.
     *
     * This method should be exposed in the static constructor of the subclass.
     *
     * @param fragment - The fragment to push into the buffer.
     */
    protected push(fragment: TFragment): void;
    protected constructor();
    then<TResult1 = TFinal, TResult2 = never>(onfulfilled?: ((value: TFinal) => TResult1 | PromiseLike<TResult1>) | null | undefined, onrejected?: ((reason: any) => TResult2 | PromiseLike<TResult2>) | null | undefined): Promise<TResult1 | TResult2>;
    catch<TResult = never>(onrejected?: ((reason: any) => TResult | PromiseLike<TResult>) | null | undefined): Promise<TFinal | TResult>;
    finally(onfinally?: (() => void) | null | undefined): Promise<TFinal>;
    [Symbol.toStringTag]: string;
    /**
     * If nextFragmentPromiseBundle exists, it is returned. Otherwise, a new one is created and
     * returned.
     */
    private obtainNextFragmentPromiseBundle;
    [Symbol.asyncIterator](): AsyncIterator<TFragment, any, undefined>;
}

/** @public */
export declare class SystemNamespace {
    private readonly systemPort;
    /**
     * List all downloaded models.
     * @public
     */
    listDownloadedModels(): Promise<Array<DownloadedModel>>;
}

export { }
